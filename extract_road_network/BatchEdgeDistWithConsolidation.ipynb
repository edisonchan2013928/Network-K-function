{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import math\n",
    "import osmnx as ox\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry import LineString\n",
    "# from distance import nearest_edges as getNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "# G1 = ox.graph_from_place('Chicago, Cook County, Illinois, United States', network_type='drive') # full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = ox.plot_graph(G1)\n",
    "# G1.nodes[39076461]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gp1 = ox.project_graph(G1) # need to do projection before consolidation, do projection also for query points!\n",
    "# Gc1 = ox.consolidate_intersections(Gp1, tolerance=20, rebuild_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gc1.nodes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrepareNodeidDict(G):\n",
    "    \n",
    "    '''\n",
    "    This function is used to handle the 'id-0, id-1, id-2..' scenario appeared in nodeid after applying consolidate_intersection\n",
    "    Parameter:\n",
    "    G: after applying consolidate_intersection\n",
    "    '''\n",
    "    \n",
    "    nodeid_dict = {}\n",
    "    \n",
    "    new_id = 0\n",
    "    for origin_id in G:\n",
    "        if type(origin_id) == int:\n",
    "            nodeid_dict.update({origin_id:origin_id})\n",
    "            new_id = origin_id\n",
    "        else:\n",
    "            new_id += 1\n",
    "            nodeid_dict.update({origin_id:new_id})\n",
    "            \n",
    "    return nodeid_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate the edges in the graph into format:\n",
    "#   - edge_id, node1_id, node2_id, length\n",
    "#\n",
    "# Parameters:\n",
    "# - G: the OSMnx road graph, projected, consolidated\n",
    "# - path_to: save the edge format to\n",
    "#\n",
    "# Return:\n",
    "# - edge dictionary?\n",
    "#\n",
    "\n",
    "def ProcessEdges(G, path_to, nodeid_dict):\n",
    "    \n",
    "    edge_dict = {}\n",
    "    edge_list = []\n",
    "    edge_np = np.array(G.edges)\n",
    "    \n",
    "    for i, edge in enumerate(G.edges):\n",
    "        \n",
    "        if i % 10000 == 0:\n",
    "            print(\"current edge: \", i)\n",
    "            \n",
    "        # using the origin node format for calculation\n",
    "        origin_node1_id = edge[0]\n",
    "        origin_node2_id = edge[1]\n",
    "        #length_meters = nx.shortest_path_length(G, source=origin_node1_id, target=origin_node2_id, weight='length')\n",
    "        length_meters = G[origin_node1_id][origin_node2_id][0]['length']\n",
    "        \n",
    "        # used the save node format for recording\n",
    "        new_node1_id = nodeid_dict[origin_node1_id]\n",
    "        new_node2_id = nodeid_dict[origin_node2_id]\n",
    "        \n",
    "        # the projected location is newly added, # edge_id, node1_id, node2_id, length in meters\n",
    "        edge_list.append([i, new_node1_id, new_node2_id, length_meters, G.nodes[origin_node1_id]['x'], G.nodes[origin_node1_id]['y'], G.nodes[origin_node2_id]['x'], G.nodes[origin_node2_id]['y']]) \n",
    "        \n",
    "        edge_dict.update({(new_node1_id, new_node2_id):i}) # using 2 node_id as edge's key, using edge id as value\n",
    "        edge_dict.update({(new_node2_id, new_node1_id):-i}) # indicate the inverse\n",
    "        if i == 0:\n",
    "            edge_dict.update({(new_node2_id, new_node1_id):-0.1}) # special case for zero\n",
    "        \n",
    "    edge_list_np = np.array(edge_list)\n",
    "    np.savetxt(path_to, edge_list_np, delimiter=',')\n",
    "    \n",
    "    edge_list_np = edge_list_np[:,0:4] # we don't need the remaining info\n",
    "    \n",
    "    return edge_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EuclideanDist(p1x, p1y, p2x, p2y):\n",
    "    return math.sqrt((p1x - p2x)**2 + (p1y - p2y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetDistancesOptimized(G, path_from, path_to, nodeid_dict):\n",
    "    \n",
    "    '''\n",
    "    Parameters:\n",
    "    @path_from: the query file\n",
    "    @G: projected, consolidated road network\n",
    "    '''\n",
    "    # get the nodes stored in csv \n",
    "    nodes = np.genfromtxt(path_from, delimiter=' ')\n",
    "\n",
    "    \n",
    "    longitudes = nodes[:,0]\n",
    "    latitudes = nodes[:,1]\n",
    "\n",
    "    \n",
    "    points_list = [Point((lng, lat)) for lat, lng in zip(latitudes, longitudes)] # turn into shapely geometry\n",
    "    points = gpd.GeoSeries(points_list, crs='epsg:4326') # turn into geoseries, with default crs, i.e., ox.settings.default_crs\n",
    "    points_proj = points.to_crs(G.graph['crs'])\n",
    "    \n",
    "    xs = [pp.x for pp in points_proj]\n",
    "    ys = [pp.y for pp in points_proj]\n",
    "\n",
    "   \n",
    "    \n",
    "    # find the nearest edges for all query points\n",
    "    nearest_edges = ox.distance.get_nearest_edges(G, xs, ys, method='kdtree', dist=1) # dist = 1 for meters\n",
    "    # print(\"xs: \", xs)\n",
    "    # print(\"ys: \", ys)\n",
    "    # nearest_edges = getNE(G, xs, ys, interpolate=1000.0)\n",
    "    print(\"finished finding all the nearest edges\")\n",
    "    \n",
    "    distances = []\n",
    "    \n",
    "    # find distances\n",
    "    for i in range(len(nodes)):\n",
    "        if i % 10000 == 0:\n",
    "            print(\"current point: \", i)\n",
    "            \n",
    "        # edge's 2 ends's node id\n",
    "        point1_id = nearest_edges[i][0] # the nearest edge'S first end's node id\n",
    "        point2_id = nearest_edges[i][1] # the nearest edge'S second end's node id\n",
    "\n",
    "        # generate projection on nearest edge\n",
    "        #ref_point = Point(nodes[i,1], nodes[i,0]) # Point(lon, lat)\n",
    "        ref_point = Point(xs[i], ys[i]) # the projected point\n",
    "        \n",
    "        point_1 = Point(G.nodes[point1_id]['x'], G.nodes[point1_id]['y']) # x is longitude, y is latitude\n",
    "        point_2 = Point(G.nodes[point2_id]['x'], G.nodes[point2_id]['y'])\n",
    "\n",
    "        # this is the distance from one end to the projected position (but we don't know the other end)\n",
    "        projected_dist = LineString([point_1, point_2]).project(ref_point)\n",
    "        # print(\"projected_dist: \", projected_dist)\n",
    "\n",
    "\n",
    "        # get the projected point\n",
    "        projected_point = list(LineString([point_1, point_2]).interpolate(projected_dist).coords)\n",
    "        # print(\"projected_point: \", projected_point)\n",
    "    \n",
    "            \n",
    "        \n",
    "        # using Haversine(OSMnx's)\n",
    "        #distance_1 = ox.distance.great_circle_vec(projected_point[0][1], projected_point[0][0], G.nodes[point1_id]['y'], G.nodes[point1_id]['x'])\n",
    "        #distance_2 = ox.distance.great_circle_vec(projected_point[0][1], projected_point[0][0], G.nodes[point2_id]['y'], G.nodes[point2_id]['x'])\n",
    "        distance_1 = EuclideanDist(projected_point[0][0], projected_point[0][1], G.nodes[point1_id]['x'], G.nodes[point1_id]['y'])\n",
    "        distance_2 = EuclideanDist(projected_point[0][0], projected_point[0][1], G.nodes[point2_id]['x'], G.nodes[point2_id]['y'])\n",
    "        \n",
    "        # used the save node format for recording\n",
    "        new_node1_id = nodeid_dict[point1_id]\n",
    "        new_node2_id = nodeid_dict[point2_id]\n",
    "        \n",
    "        # the query node's closest edge's two nodes and its distance\n",
    "        distances.append([new_node1_id, new_node2_id, distance_1, distance_2])\n",
    "    \n",
    "    distances_np = np.array(distances)\n",
    "    np.savetxt(path_to, distances_np, delimiter=',')\n",
    "    \n",
    "    return distances_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the distance into edison's point format:\n",
    "#  - edge_id, distance_1, distance_2\n",
    "#\n",
    "# Parameters:\n",
    "#  - distances: the ourput results from GetDistances()\n",
    "#  - edge_dict: the dictionary from ProcessEdges()\n",
    "#  - path_to: save path\n",
    "#\n",
    "# Return:\n",
    "#  - the processed PointFormat\n",
    "#\n",
    "def ConvertFormat(distances, edge_dict, path_to):\n",
    "    \n",
    "    processed_distances = []\n",
    "    \n",
    "    for i in range(len(distances)):\n",
    "        edge_node_pair = (distances[i][0], distances[i][1])\n",
    "        edge_key = edge_dict[edge_node_pair]\n",
    "        \n",
    "        if edge_key >= 0:\n",
    "            processed_distances.append([edge_key, distances[i][2], distances[i][3]])\n",
    "        else:\n",
    "            # change the order\n",
    "            edge_key = int(abs(edge_key)) # process the edge_key\n",
    "            processed_distances.append([edge_key, distances[i][3], distances[i][2]])\n",
    "                \n",
    "    processed_distances_np = np.array(processed_distances)\n",
    "    np.savetxt(path_to, processed_distances_np, delimiter=',')\n",
    "    \n",
    "    return processed_distances_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === execution (New York) ===\n",
    "\n",
    "# G1 = ox.graph_from_place('London, Greater London, England, United Kingdom', network_type='drive') # full\n",
    "\n",
    "# G1 = ox.graph_from_place('Chicago, Cook County, Illinois, USA', network_type='drive') # full\n",
    "\n",
    "# G1 = ox.graph_from_place('Detroit, Wayne County, Michigan, USA', network_type='drive') # full\n",
    "\n",
    "G1 = ox.graph_from_place('San Francisco, California, USA', network_type='drive') # full\n",
    "\n",
    "Gp1 = ox.project_graph(G1) # need to do projection before consolidation, do projection also for query points!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tolerance of London=46, and 20 for other datasets!!!!\n",
    "Gc1 = ox.consolidate_intersections(Gp1, tolerance=20, rebuild_graph=True) \n",
    "nodeid_dict1 = PrepareNodeidDict(Gc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "node num:  16288\nedge num:  51845\n"
     ]
    }
   ],
   "source": [
    "print(\"node num: \", Gc1.number_of_nodes())\n",
    "print(\"edge num: \", Gc1.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_name = \"Detroit\" # London_cut, Chicago, Detroit, San_Francisco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "node num:  16288\n",
      "edge num:  51845\n",
      "current edge:  0\n",
      "current edge:  10000\n",
      "current edge:  20000\n",
      "current edge:  30000\n",
      "current edge:  40000\n",
      "current edge:  50000\n"
     ]
    }
   ],
   "source": [
    "print(\"node num: \", Gc1.number_of_nodes())\n",
    "print(\"edge num: \", Gc1.number_of_edges())\n",
    "edge_dict1 = ProcessEdges(Gc1, \"results/\"+city_name+\"_clean.edge\", nodeid_dict1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pos [ 134515 3968722 6459035 ... 2178924 1196574 4765979]\n",
      "finished finding all the nearest edges\n",
      "current point:  0\n",
      "current point:  10000\n",
      "current point:  20000\n",
      "current point:  30000\n",
      "current point:  40000\n",
      "current point:  50000\n",
      "current point:  60000\n",
      "current point:  70000\n",
      "current point:  80000\n",
      "current point:  90000\n",
      "current point:  100000\n",
      "current point:  110000\n",
      "current point:  120000\n",
      "current point:  130000\n",
      "current point:  140000\n",
      "current point:  150000\n",
      "current point:  160000\n",
      "current point:  170000\n",
      "current point:  180000\n",
      "current point:  190000\n",
      "current point:  200000\n",
      "current point:  210000\n",
      "current point:  220000\n",
      "current point:  230000\n",
      "current point:  240000\n",
      "current point:  250000\n",
      "current point:  260000\n",
      "current point:  270000\n",
      "current point:  280000\n",
      "current point:  290000\n",
      "current point:  300000\n",
      "current point:  310000\n",
      "current point:  320000\n",
      "current point:  330000\n",
      "current point:  340000\n",
      "current point:  350000\n",
      "current point:  360000\n",
      "current point:  370000\n",
      "current point:  380000\n",
      "current point:  390000\n",
      "current point:  400000\n",
      "current point:  410000\n",
      "current point:  420000\n",
      "current point:  430000\n",
      "current point:  440000\n",
      "current point:  450000\n",
      "current point:  460000\n",
      "current point:  470000\n",
      "current point:  480000\n",
      "current point:  490000\n",
      "current point:  500000\n",
      "current point:  510000\n",
      "current point:  520000\n",
      "current point:  530000\n",
      "current point:  540000\n",
      "current point:  550000\n",
      "current point:  560000\n",
      "current point:  570000\n",
      "current point:  580000\n",
      "current point:  590000\n",
      "current point:  600000\n",
      "current point:  610000\n",
      "current point:  620000\n",
      "current point:  630000\n",
      "current point:  640000\n",
      "current point:  650000\n",
      "current point:  660000\n",
      "current point:  670000\n",
      "current point:  680000\n",
      "current point:  690000\n",
      "current point:  700000\n",
      "current point:  710000\n",
      "current point:  720000\n",
      "current point:  730000\n",
      "current point:  740000\n",
      "current point:  750000\n",
      "current point:  760000\n",
      "current point:  770000\n",
      "current point:  780000\n",
      "current point:  790000\n",
      "current point:  800000\n",
      "current point:  810000\n",
      "current point:  820000\n",
      "current point:  830000\n",
      "current point:  840000\n",
      "current point:  850000\n",
      "current point:  860000\n",
      "current point:  870000\n",
      "current point:  880000\n",
      "current point:  890000\n",
      "current point:  900000\n",
      "current point:  910000\n",
      "current point:  920000\n",
      "current point:  930000\n",
      "current point:  940000\n",
      "current point:  950000\n",
      "current point:  960000\n",
      "current point:  970000\n",
      "current point:  980000\n",
      "current point:  990000\n",
      "current point:  1000000\n",
      "current point:  1010000\n",
      "current point:  1020000\n",
      "current point:  1030000\n",
      "current point:  1040000\n",
      "current point:  1050000\n",
      "current point:  1060000\n",
      "current point:  1070000\n",
      "current point:  1080000\n",
      "current point:  1090000\n",
      "current point:  1100000\n",
      "current point:  1110000\n",
      "current point:  1120000\n",
      "current point:  1130000\n",
      "current point:  1140000\n",
      "current point:  1150000\n",
      "current point:  1160000\n",
      "current point:  1170000\n",
      "current point:  1180000\n",
      "current point:  1190000\n",
      "current point:  1200000\n",
      "current point:  1210000\n",
      "current point:  1220000\n",
      "current point:  1230000\n",
      "current point:  1240000\n",
      "current point:  1250000\n",
      "current point:  1260000\n",
      "current point:  1270000\n",
      "current point:  1280000\n",
      "current point:  1290000\n",
      "current point:  1300000\n",
      "current point:  1310000\n",
      "current point:  1320000\n",
      "current point:  1330000\n",
      "current point:  1340000\n",
      "current point:  1350000\n",
      "current point:  1360000\n",
      "current point:  1370000\n",
      "current point:  1380000\n",
      "current point:  1390000\n",
      "current point:  1400000\n",
      "current point:  1410000\n",
      "current point:  1420000\n",
      "current point:  1430000\n",
      "current point:  1440000\n",
      "current point:  1450000\n",
      "current point:  1460000\n",
      "current point:  1470000\n",
      "current point:  1480000\n",
      "current point:  1490000\n",
      "current point:  1500000\n",
      "current point:  1510000\n",
      "current point:  1520000\n",
      "current point:  1530000\n",
      "current point:  1540000\n",
      "current point:  1550000\n",
      "current point:  1560000\n",
      "current point:  1570000\n",
      "current point:  1580000\n",
      "current point:  1590000\n",
      "current point:  1600000\n",
      "current point:  1610000\n",
      "current point:  1620000\n",
      "current point:  1630000\n",
      "current point:  1640000\n",
      "current point:  1650000\n",
      "current point:  1660000\n",
      "current point:  1670000\n",
      "current point:  1680000\n",
      "current point:  1690000\n",
      "current point:  1700000\n",
      "current point:  1710000\n",
      "current point:  1720000\n",
      "current point:  1730000\n",
      "current point:  1740000\n",
      "current point:  1750000\n",
      "current point:  1760000\n",
      "current point:  1770000\n",
      "current point:  1780000\n",
      "current point:  1790000\n",
      "current point:  1800000\n",
      "current point:  1810000\n",
      "current point:  1820000\n",
      "current point:  1830000\n",
      "current point:  1840000\n",
      "current point:  1850000\n",
      "current point:  1860000\n",
      "current point:  1870000\n",
      "current point:  1880000\n",
      "current point:  1890000\n",
      "current point:  1900000\n",
      "current point:  1910000\n",
      "current point:  1920000\n",
      "current point:  1930000\n",
      "--- 658.6491544246674 seconds ---\n"
     ]
    }
   ],
   "source": [
    "for sample_ratio in [1.0]: #[0.25, 0.5, 0.75, 1.0]:\n",
    "    start_time = time.time()\n",
    "\n",
    "    # query_set1 = \"Datasets_Yun/\"+city_name+\"_clean_s\"+str(sample_ratio)+\".csv\"\n",
    "    # distances1 = GetDistancesOptimized(Gc1, query_set1, \"results/\"+city_name+\"_clean_s\"+str(sample_ratio)+\".distances\", nodeid_dict1)\n",
    "\n",
    "    query_set1 = \"Datasets_Yun/\"+city_name+\"_clean.csv\"\n",
    "    distances1 = GetDistancesOptimized(Gc1, query_set1, \"results/\"+city_name+\"_clean.distances\", nodeid_dict1)\n",
    "    \n",
    "    # processed_distance1 = ConvertFormat(distances1, edge_dict1, \"results/\"+city_name+\"_clean_s\"+str(sample_ratio)+\".processed.distances\")\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # === execution (Atlanta) ===\n",
    "# start_time = time.time()\n",
    "# # place2_road_full = ox.graph_from_place('Atlanta, Georgia', network_type='drive') # full, this do not contains the query points\n",
    "\n",
    "# # bounding box for City of Johns Creek, \n",
    "# bbox = (34.089791, 33.990863, -84.097653, -84.281025) # latitude high, latitude low, longitude high, longitude low\n",
    "# G2 = ox.graph_from_bbox(*bbox, network_type='drive')\n",
    "# Gp2 = ox.project_graph(G2) # need to do projection before consolidation, do projection also for query points!\n",
    "# Gc2 = ox.consolidate_intersections(Gp2, tolerance=20, rebuild_graph=True)\n",
    "\n",
    "# nodeid_dict2 = PrepareNodeidDict(Gc2)\n",
    "# edge_dict2 = ProcessEdges(Gc2, \"results/Atlanta_edges\", nodeid_dict2)\n",
    "# query_set2 = \"Datasets_Zhe/Atlanta_police_calls_ours\"\n",
    "# distances2 = GetDistancesOptimized(Gc2, query_set2, \"results/Atlanta_distances\", nodeid_dict2)\n",
    "# processed_distance2 = ConvertFormat(distances2, edge_dict2, \"results/Atlanta_processed_distances\")\n",
    "# print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # === execution (Los Angeles) ===\n",
    "# start_time = time.time()\n",
    "# G3 = ox.graph_from_place('Los Angeles, California', network_type='drive') # full\n",
    "# Gp3 = ox.project_graph(G3) # need to do projection before consolidation, do projection also for query points!\n",
    "# Gc3 = ox.consolidate_intersections(Gp3, tolerance=20, rebuild_graph=True)\n",
    "# del G3\n",
    "# del Gp3\n",
    "# nodeid_dict3 = PrepareNodeidDict(Gc3)\n",
    "# edge_dict3 = ProcessEdges(Gc3, \"results/Los_Angeles_edges\", nodeid_dict3)\n",
    "# query_set3 = \"Datasets_Zhe/Los_Angles_crime_ours\"\n",
    "# distances3 = GetDistancesOptimized(Gc3, query_set3, \"results/Los_Angeles_distances\", nodeid_dict3)\n",
    "# processed_distance3 = ConvertFormat(distances3, edge_dict3, \"results/Los_Angeles_processed_distances\")\n",
    "# print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # === execution (Seattle) ===\n",
    "# start_time = time.time()\n",
    "# G4 = ox.graph_from_place('Seattle, Washington', network_type='drive')  # full\n",
    "# Gp4 = ox.project_graph(G4) # need to do projection before consolidation, do projection also for query points!\n",
    "# Gc4 = ox.consolidate_intersections(Gp4, tolerance=20, rebuild_graph=True)\n",
    "# nodeid_dict4 = PrepareNodeidDict(Gc4)\n",
    "# edge_dict4 = ProcessEdges(Gc4, \"results/Seattle_edges\", nodeid_dict4)\n",
    "# query_set4 = \"Datasets_Zhe/Seattle_crime_ours\"\n",
    "# distances4 = GetDistancesOptimized(Gc4, query_set4, \"results/Seattle_distances\", nodeid_dict4)\n",
    "# processed_distance4 = ConvertFormat(distances4, edge_dict4, \"results/Seattle_processed_distances\")\n",
    "# print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('osmnx': conda)",
   "language": "python",
   "name": "python39764bitosmnxconda7585c0c1aef240e6b6fb41767f006ce4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}